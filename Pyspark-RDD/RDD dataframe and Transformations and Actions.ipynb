{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c618df81",
   "metadata": {},
   "source": [
    "# Pyspark With RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1a923a",
   "metadata": {},
   "source": [
    "* RDD (Resilient Distributed Dataset) is the fundamental data structure of Apache Spark\n",
    "* which are an immutable collection of objects which computes on the different node of the cluster. \n",
    "* Each and every dataset in Spark RDD is logically partitioned across many servers so that they can be computed on different nodes of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1682c67",
   "metadata": {},
   "source": [
    "* Resilient :-- fault-tolerant with the help of RDD lineage graph(DAG) and so able to recompute missing or damaged partitions due to node failures.\n",
    "* Distributed :-- since Data resides on multiple nodes.\n",
    "* Dataset :-- represents records of the data you work with. The user can load the data set externally which can be either JSON file, CSV file, text file or database via JDBC with no specific data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac52519",
   "metadata": {},
   "source": [
    "# Features of Spark RDD\n",
    "\n",
    "#### 1.In-memory Computation\n",
    "\n",
    " Spark RDDs have a provision of in-memory computation. \n",
    " It stores intermediate results in distributed memory(RAM) instead of stable storage(disk).\n",
    "\n",
    "#### 2.Lazy Evaluations  \n",
    "\n",
    "All transformations in Apache Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base data set.\n",
    "\n",
    "Spark computes transformations when an action requires a result for the driver program. Follow this guide for the deep study of Spark Lazy Evaluation.\n",
    "\n",
    "#### 3.Fault Tolerance\n",
    "\n",
    "Spark RDDs are fault tolerant as they track data lineage information to rebuild lost data automatically on failure. They rebuild lost data on failure using lineage, each RDD remembers how it was created from other datasets (by transformations like a map, join or groupBy) to recreate itself. Follow this guide for the deep study of RDD Fault Tolerance.\n",
    "\n",
    "#### 4.Immutability\n",
    "\n",
    "Data is safe to share across processes. It can also be created or retrieved anytime which makes caching, sharing & replication easy. Thus, it is a way to reach consistency in computations.\n",
    "\n",
    "#### 5.Partitioning\n",
    "\n",
    "Partitioning is the fundamental unit of parallelism in Spark RDD. Each partition is one logical division of data which is mutable. One can create a partition through some transformations on existing partitions.\n",
    "\n",
    "#### 6.Persistence\n",
    "\n",
    "Users can state which RDDs they will reuse and choose a storage strategy for them (e.g., in-memory storage or on Disk).\n",
    "\n",
    "#### 7.Coarse-grained Operations\n",
    "\n",
    "It applies to all elements in datasets through maps or filter or group by operation.\n",
    "\n",
    "\n",
    "#### 8.Location-Stickiness\n",
    "\n",
    "RDDs are capable of defining placement preference to compute partitions. Placement preference refers to information about the location of RDD. The DAGScheduler places the partitions in such a way that task is close to data as much as possible. Thus, speed up computation. Follow this guide to learn What is DAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf72def",
   "metadata": {},
   "source": [
    "## RDD in Apache Spark supports two types of operations:\n",
    "\n",
    "#### 1.Transformation\n",
    "Spark RDD Transformations are functions that take an RDD as the input and produce one or many RDDs as the output. They do not change the input RDD (since RDDs are immutable and hence one cannot change it), but always produce one or more new RDDs by applying the computations they represent e.g. Map(), filter(), reduceByKey() etc.\n",
    "* a. Narrow Transformations \n",
    "     * It is the result of map, filter and such that the data is from a single partition only, i.e. it is self-sufficient. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result.Spark groups narrow transformations as a stage known as pipelining.\n",
    "* b. Wide Transformations\n",
    "    * It is the result of groupByKey() and reduceByKey() like functions. The data required to compute the records in a single partition may live in many partitions of the parent RDD. Wide transformations are also known as shuffle transformations because they may or may not depend on a shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab85302",
   "metadata": {},
   "source": [
    "#### 2.Actions\n",
    "* An Action in Spark returns final result of RDD computations. It triggers execution using lineage graph to load the data into original RDD, carry out all intermediate transformations and return final results to Driver program or write it out to file system. Lineage graph is dependency graph of all parallel RDDs of RDD.\n",
    "\n",
    "* Actions are RDD operations that produce non-RDD values. They materialize a value in a Spark program. An Action is one of the ways to send result from executors to the driver. First(), take(), reduce(), collect(), the count() is some of the Actions in spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75551797",
   "metadata": {},
   "source": [
    "# Creating RDD one way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44c685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be3ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or with \n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef3bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([(1,'vineeth',500,'A'),\n",
    "                                        (2,'sudhanshu',450,'B'),\n",
    "                                         (3,'krish Naik',500,'A'),\n",
    "                                         (4,'sunny',350,'C'),\n",
    "                                         (5,'hitesh',600,'A+'),\n",
    "                                         (6,'himesh',500,'A'),\n",
    "                                         (7,'naveen',550,'A+'),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aadee3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'vineeth', 500, 'A'),\n",
       " (2, 'sudhanshu', 450, 'B'),\n",
       " (3, 'krish Naik', 500, 'A'),\n",
       " (4, 'sunny', 350, 'C'),\n",
       " (5, 'hitesh', 600, 'A+'),\n",
       " (6, 'himesh', 500, 'A'),\n",
       " (7, 'naveen', 550, 'A+')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51003f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## type of data\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8ed0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have to create a dataframe with schaema in rdd but i won't rdd have a schema\n",
    "df_type_rdd = spark.createDataFrame(rdd,schema=[\"Id\",\"name\",\"marks\",\"grade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867f6924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=1, name='vineeth', marks=500, grade='A'),\n",
       " Row(Id=2, name='sudhanshu', marks=450, grade='B'),\n",
       " Row(Id=3, name='krish Naik', marks=500, grade='A'),\n",
       " Row(Id=4, name='sunny', marks=350, grade='C'),\n",
       " Row(Id=5, name='hitesh', marks=600, grade='A+'),\n",
       " Row(Id=6, name='himesh', marks=500, grade='A'),\n",
       " Row(Id=7, name='naveen', marks=550, grade='A+')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_type_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73d47220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_type_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6fbb585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+\n",
      "|Id |name      |marks|grade|\n",
      "+---+----------+-----+-----+\n",
      "|1  |vineeth   |500  |A    |\n",
      "|2  |sudhanshu |450  |B    |\n",
      "|3  |krish Naik|500  |A    |\n",
      "|4  |sunny     |350  |C    |\n",
      "|5  |hitesh    |600  |A+   |\n",
      "|6  |himesh    |500  |A    |\n",
      "|7  |naveen    |550  |A+   |\n",
      "+---+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show ths dataset\n",
    "df_type_rdd.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243b0b2",
   "metadata": {},
   "source": [
    "## RDD Narrow Tramsformation Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15511bc3",
   "metadata": {},
   "source": [
    "* Map\n",
    "* flatmap\n",
    "* Mappartion\n",
    "* filter\n",
    "* sample\n",
    "* union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "902530c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7b5a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6771b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When \"data\" is used more generally as a synonym for \"information\", it is treated as a mass noun in singular form. This usage is common in everyday language and in technical and scientific fields such as software development and computer science. One example of this usage is the term \"big data\". When used more specifically to refer to the processing and analysis of sets of data, the term retains its plural form. This usage is common in natural sciences, life sciences, social sciences, software development and computer science, and grew in popularity in the 20th and 21st centuries. Some style guides do not recognize the different meanings of the term, and simply recommend the form that best suits the target audience of the guide. For example, APA style as of the 7th edition requires \"data\" to be treated as a plural form.[7].']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "091e8a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e3b6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e6cd85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8ca8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = dff.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7af2d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When \"data\" is used more generally as a synonym for \"information\"',\n",
       " ' it is treated as a mass noun in singular form. This usage is common in everyday language and in technical and scientific fields such as software development and computer science. One example of this usage is the term \"big data\". When used more specifically to refer to the processing and analysis of sets of data',\n",
       " ' the term retains its plural form. This usage is common in natural sciences',\n",
       " ' life sciences',\n",
       " ' social sciences',\n",
       " ' software development and computer science',\n",
       " ' and grew in popularity in the 20th and 21st centuries. Some style guides do not recognize the different meanings of the term',\n",
       " ' and simply recommend the form that best suits the target audience of the guide. For example',\n",
       " ' APA style as of the 7th edition requires \"data\" to be treated as a plural form.[7].']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba7e9694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71042b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "some examples in narrow transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac97ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map function\n",
    "x_map = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "y_map = x_map.map(lambda x: (x,x**2))\n",
    "print(x_map.collect())\n",
    "print(y_map.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "## flatMap function\n",
    "x_map = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "y_map = x_map.flatMap(lambda x: (x,x**2,100*x))\n",
    "print(x_map.collect())\n",
    "print(y_map.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b4ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter elements with comparision operation\n",
    "x_filter = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "y_filter_rdd= x_filter.filter(lambda x: x%2 ==0)\n",
    "print(x_filter.collect())\n",
    "print(y_filter_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575aa370",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mappartions is divide with 3 nodes and sum of each node is mapPartitions\n",
    "x_partion = sc.parallelize([1,2,3,4,5,6,7,8,9],3)\n",
    "def f(iterator):yield sum(iterator)\n",
    "y = x_partion.mapPartitions(f)\n",
    "print(x_partion.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ed596",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapPartitionWithIndex\n",
    "x_mpwi = sc.parallelize([1,2,3,4,5,6,7,8,9],3)\n",
    "def f(partitionIndex,iterator):yield (partitionIndex,sum(iterator))\n",
    "y_mpwi= x_mpwi.mapPartitionsWithIndex(f)\n",
    "print(x_mpwi.glom().collect())\n",
    "print(y_mpwi.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample function\n",
    "x_samp = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "y_samp = x_samp.sample(False,0.4)  # withreplacement is false at that time it wotn repeat same numbers \n",
    "print(x_samp.collect())\n",
    "print(y_samp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1672b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## union is the mainly used to join the two rdd data elements\n",
    "rdd1 = sc.parallelize([1,2,3,4,5,6,7])\n",
    "rdd2 = sc.parallelize([1,2,3,3,4,4,5])\n",
    "rdd1.union(rdd2).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
